{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jCwEJ3SoF74"
   },
   "source": [
    "# 원불교 교전 데이터 대상 BertWordPieceTokenizer 훈련\n",
    "\n",
    "## 1. 개요\n",
    "\n",
    "원불교 교전 데이터를 대상으로 토크나이저를 훈련합니다.\n",
    "\n",
    "* 데이터셋:\n",
    "  * 원불교 교전\n",
    "* 토크나이저:\n",
    "  * 훈련: 허깅 페이스 [Tokenizers](https://huggingface.co/docs/tokenizers/index) 라이브러리의 [BertWordPieceTokenizer](https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/bert_wordpiece.py) 클래스\n",
    "  * 사용: 허깅 페이스 [Transformers](https://huggingface.co/docs/transformers/index) 라이브러리의 [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDygGqtmppxP"
   },
   "source": [
    "## 2. 말뭉치 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M1nZYAwvpv_0"
   },
   "outputs": [],
   "source": [
    "corpus_file = \"./won/data/won04-gyojeon.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q9AzQb-pxW8"
   },
   "source": [
    "말뭉치 파일에 대하여 아래 정보를 출력합니다.\n",
    "\n",
    "* 총 줄 수\n",
    "* 총 단어 수(단어를 공백으로 구분할 때)\n",
    "* 가장 많은 단어를 포함하고 있는 줄의 단어 수\n",
    "* 가장 많은 단어를 포함하고 있는 줄의 텍스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Nt9WQUip65Z",
    "outputId": "4cf7d740-87f0-4a58-bcef-d382fe4bd195"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 파일 분석 결과 (./won/data/won04-gyojeon.txt) =====\n",
      ">> 총 줄 수  :    1,082\n",
      ">> 총 단어 수:   59,541\n",
      ">> 가장 많은 단어를 포함하고 있는 줄:\n",
      "* 줄 위치:      804\n",
      "* 단어 수:      362\n",
      "* 텍스트 :\n",
      "-----\n",
      "17.한 사람이 대종사께 뵈옵고 여러 가지로 담화하는 가운데 [전주․이리 사이의 경편철도(輕便鐵道)는 본래 전라도 각지의 부호들이 주식 출자로 경영하는 것이라, 그들은 언제나 그 경편차를 무료로 이용하고 다닌다.] 하면서 매우 부러워하는 태도를 보이거늘, 대종사 말씀하시기를 [그대는 참으로 가난하도다. 아직 그 차 하나를 그대의 소유로 삼지 못하였는가.] 그 사람이 놀라 여쭙기를 [경편차 하나를 소유하자면 상당한 돈이 있어야 할 것이온데 이 같은 무산자로서 어떻게 그것을 소유할 수 있사오리까.] 대종사 말씀하시기를 [그러므로, 그대를 가난한 사람이라 하였으며, 설사 그대가 경편차 하나를 소유하였다 할지라도 나는 그것으로 그대를 부유한 사람이라고는 아니할 것이니, 이제 나의 살림하는 이야기를 좀 들어보라. 나는 저 전주 경편차뿐 아니라 나라 안의 차와 세계의 모든 차까지도 다 내 것을 삼은지가 벌써 오래 되었노니, 그대는 이 소식을 아직도 모르는가.] 그 사람이 더욱 놀라 사뢰기를 [그 말씀은 실로 요량 밖의 교훈이시므로 어리석은 소견으로는 그 뜻을 살피지 못하겠나이다.] 대종사 말씀하시기를 [사람이 기차 하나를 자기의 소유로 하려면 거액(巨額)의 자금이 일시에 들어야 할 것이요, 운영하는 모든 책임을 직접 담당하여 많은 괴로움을 받아야 할 것이나, 나의 소유하는 법은 그와 달라서 단번에 거액을 들이지도 아니하며, 모든 운영의 책임을 직접 지지도 아니하고, 다만 어디를 가게 되면 그 때마다 얼마씩의 요금만 지불하고 나의 마음대로 이용하는 것이니, 주야로 쉬지 않고 우리 차를 운전하며, 우리 철도를 수선하며, 우리 사무를 관리하여 주는 모든 우리 일꾼들의 급료와 비용이 너무 싸지 아니한가. 또, 나는 저번에 서울에 가서 한양 공원에 올라가 산책하면서 맑은 공기를 한 없이 호흡도 하고 온 공원의 흥취를 다 같이 즐기기도 하였으되, 누가 우리를 가라는 법도 없고 다시 오지 말라는 말도 아니하였나니, 피서 지대에 정자 몇 간만 두어도 매년 적지 않은 수호비가 들 것인데, 우리는 그러지 아니하고도 그 좋은 공원을 충분히 내 것으로 이용하지 아니 하였는가. 대저, 세상 사람이 무엇이나 제 것을 삼으려는 본의는 다 자기의 편리를 취함이어늘 기차나 공원을 모두 다 이와 같이 이용할 대로 이용하였으니 어떻게 소유한들 이 위에 더 나은 방법이 있겠는가. 그러므로, 나는 이것을 모두 다 내 것이라고 하였으며, 그뿐 아니라 세상의 모든 것과 그 모든 것을 싣고 있는 대지 강산까지도 다 내 것을 삼아 두고, 경우에 따라 그것을 이용하되 경위에만 어긋나지 않게 하면 아무도 금하고 말리지 못하나니, 이 얼마나 너른 살림인가. 그러나, 속세 범상한 사람들은 기국(器局)이 좁아서 무엇이나 기어이 그것을 자기 앞에 갖다 놓기로만 위주하여 공연히 일 많고 걱정되고 책임 무거울 것을 취하기에 급급하나니, 이는 참으로 국한 없이 큰 본가 살림을 발견하지 못한 연고니라.]\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def count_lines_and_words(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            line_count = len(lines)\n",
    "            word_count = 0\n",
    "            \n",
    "            max_line_pos = 0\n",
    "            max_line_length = 0\n",
    "            max_line_text = \"\"\n",
    "            \n",
    "            for i, line in enumerate(lines):\n",
    "                words = line.split() # 공백으로 단어 분리\n",
    "                line_length = len(words)\n",
    "                word_count += line_length\n",
    "                \n",
    "                if line_length > max_line_length:\n",
    "                    max_line_pos = i\n",
    "                    max_line_length = line_length\n",
    "                    max_line_text = line\n",
    "            return line_count, word_count, max_line_pos, max_line_length, max_line_text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"오류: 파일 '{filepath}'를 찾을 수 없습니다.\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# 함수 호출 및 결과 출력\n",
    "lines, words, max_line_pos, max_line_length, max_line_text = count_lines_and_words(corpus_file)\n",
    "\n",
    "if lines is not None and words is not None:\n",
    "    print(f\"===== 파일 분석 결과 ({corpus_file}) =====\")\n",
    "    print(f\">> 총 줄 수  : {lines:8,}\")\n",
    "    print(f\">> 총 단어 수: {words:8,}\")\n",
    "    print(\">> 가장 많은 단어를 포함하고 있는 줄:\")\n",
    "    print(f\"* 줄 위치: {max_line_pos:8,}\")\n",
    "    print(f\"* 단어 수: {max_line_length:8,}\")\n",
    "    print(\"* 텍스트 :\")\n",
    "    print(\"-----\")\n",
    "    print(max_line_text)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-jHm7R3qpg1"
   },
   "source": [
    "## 3. 토크나이저 훈련\n",
    "\n",
    "`BertWordPieceTokenizer`로 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "data_files = [\n",
    "    corpus_file\n",
    "]\n",
    "\n",
    "vocab_size = 7000\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "# 토크나이저 객체 생성\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=True, # False로 하면 한글 단어가 [UNK]로 토큰화됨\n",
    "    lowercase=False,\n",
    ")\n",
    "\n",
    "# 훈련\n",
    "tokenizer.train(\n",
    "    data_files,\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=min_frequency,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 결과에 `[CLS]`, `[SEP]` 토큰을 추가하는 후처리 템플릿을 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th51Srg9n-A3",
    "outputId": "cc3bd28d-53ea-4588-842e-2681a2af9fe0"
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 후처리 템플릿 지정 - 토큰화 결과에 [CLS], [SEP] 토큰 추가\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 결과를 `tokenizer.json` 파일과 `vocab.txt` 파일에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer files to ./won/tokenizers\\bert_word_piece_tokenizer_7000\n"
     ]
    }
   ],
   "source": [
    "# 파일 저장\n",
    "output_dir = \"./won/tokenizers\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "tokenizer_dir = os.path.join(output_dir, f\"bert_word_piece_tokenizer_{vocab_size}\")\n",
    "tokenizer_file = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "    \n",
    "tokenizer.save_model(tokenizer_dir) # vocab.txt 파일 저장\n",
    "tokenizer.save(tokenizer_file) # tokenizer.json 파일 저장\n",
    "\n",
    "print(f\"Saved tokenizer files to {tokenizer_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K3H592mtQl3"
   },
   "source": [
    "## 4. 토크나이저 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "He2S4FWmqnZu"
   },
   "outputs": [],
   "source": [
    "sample_texts_for_test = [\n",
    "    \"물질이 개벽되니 정신을 개벽하자\",\n",
    "    \"응용(應用)하는 데 온전한 생각으로 취사하기를 주의할 것이요\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wIh0KYExtZdI"
   },
   "outputs": [],
   "source": [
    "import sys, traceback\n",
    "\n",
    "def tokenize_and_print(tokenizer, texts):\n",
    "    for text in texts:\n",
    "        print(\"#\" * 80)\n",
    "        print(\"TEXT: '\" + text + \"'\")\n",
    "        try:\n",
    "            print(\"===== tokenizer.tokenize() =====\")\n",
    "            tokenized = tokenizer.tokenize(text)\n",
    "            print(tokenized)\n",
    "\n",
    "            print(\"===== tokenizer.encode() =====\")\n",
    "            encoded = tokenizer.encode(text)\n",
    "            print(encoded)\n",
    "            print(tokenizer.convert_ids_to_tokens(encoded))\n",
    "\n",
    "            print(\"===== tokenizer() =====\")\n",
    "            inputs = tokenizer(text)\n",
    "            print(inputs)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc(file=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r9WERDWOv4ne",
    "outputId": "fa6955ea-e651-4a08-fa8d-7a537805d946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "TEXT: '물질이 개벽되니 정신을 개벽하자'\n",
      "===== tokenizer.tokenize() =====\n",
      "['물질이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자']\n",
      "===== tokenizer.encode() =====\n",
      "[2, 4592, 5197, 1204, 1087, 1793, 5197, 1076, 1141, 3]\n",
      "['[CLS]', '물질이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자', '[SEP]']\n",
      "===== tokenizer() =====\n",
      "{'input_ids': [2, 4592, 5197, 1204, 1087, 1793, 5197, 1076, 1141, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "################################################################################\n",
      "TEXT: '응용(應用)하는 데 온전한 생각으로 취사하기를 주의할 것이요'\n",
      "===== tokenizer.tokenize() =====\n",
      "['응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요']\n",
      "===== tokenizer.encode() =====\n",
      "[2, 2239, 7, 442, 632, 8, 1241, 1343, 4070, 3551, 1917, 2128, 2568, 1242, 3]\n",
      "['[CLS]', '응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요', '[SEP]']\n",
      "===== tokenizer() =====\n",
      "{'input_ids': [2, 2239, 7, 442, 632, 8, 1241, 1343, 4070, 3551, 1917, 2128, 2568, 1242, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizerFast, DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "tokenize_and_print(tokenizer, sample_texts_for_test)\n",
    "\n",
    "# AutoTokenizer와 동일한 결과를 얻을 수 있음\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "#tokenize_and_print(tokenizer, sample_texts_for_test)\n",
    "\n",
    "# AutoTokenizer와 동일한 결과를 얻을 수 있음\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "#tokenize_and_print(tokenizer, sample_texts_for_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개의 텍스트를 하나의 쌍으로 토큰화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssb-VIoU52e-",
    "outputId": "ea8bf5d0-8a2b-45fa-f791-705e4b928e6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== tokenizer.tokenize() =====\n",
      "['물질이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자', '응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요']\n",
      "===== tokenizer.encode() =====\n",
      "[2, 4592, 5197, 1204, 1087, 1793, 5197, 1076, 1141, 3, 2239, 7, 442, 632, 8, 1241, 1343, 4070, 3551, 1917, 2128, 2568, 1242, 3]\n",
      "['[CLS]', '물질이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자', '[SEP]', '응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요', '[SEP]']\n",
      "===== tokenizer() =====\n",
      "{'input_ids': [2, 4592, 5197, 1204, 1087, 1793, 5197, 1076, 1141, 3, 2239, 7, 442, 632, 8, 1241, 1343, 4070, 3551, 1917, 2128, 2568, 1242, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '물질이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자', '[SEP]', '응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(\"===== tokenizer.tokenize() =====\")\n",
    "tokenized = tokenizer.tokenize(sample_texts_for_test[0], sample_texts_for_test[1])\n",
    "print(tokenized)\n",
    "\n",
    "print(\"===== tokenizer.encode() =====\")\n",
    "encoded = tokenizer.encode(sample_texts_for_test[0], sample_texts_for_test[1])\n",
    "print(encoded)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded))\n",
    "\n",
    "print(\"===== tokenizer() =====\")\n",
    "inputs = tokenizer(sample_texts_for_test[0], sample_texts_for_test[1])\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
