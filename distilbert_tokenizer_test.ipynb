{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52789fc3-e0d2-403d-a955-317ca8339f88",
   "metadata": {},
   "source": [
    "# DistilBERT 토크나이저 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c12babf-9f9e-42ff-8290-750ce971bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d01031-e32c-4db8-a6dc-13aa22167590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 3376, 2154,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "['[CLS]', 'beautiful', 'day', '[SEP]']\n",
      "[CLS] beautiful day [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = \"Beautiful day\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input)\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0]))\n",
    "print(tokenizer.decode(encoded_input['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ca523d-f8b0-4136-a0b9-3286706fe83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 쓰기 완료: tokenizer_pretty.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "tokenizer_dir = \"C:\\\\Users\\\\trvoi\\\\.cache\\\\huggingface\\\\hub\\\\models--distilbert-base-uncased\\\\snapshots\\\\12040accade4e8a0f71eabdb258fecc2e7e948be\"\n",
    "\n",
    "tokenizer_json_path = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
    "\n",
    "output_filename = \"tokenizer_pretty.json\"\n",
    "output_file_path = os.path.join(tokenizer_dir, output_filename)\n",
    "\n",
    "if os.path.exists(tokenizer_json_path):\n",
    "    try:\n",
    "        # tokenizer.json 읽기\n",
    "        with open(tokenizer_json_path, 'r', encoding='utf-8') as f:\n",
    "            tokenizer_config_content = json.load(f)\n",
    "        # toeknizer_pretty.json 쓰기\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as out_f:\n",
    "            json.dump(tokenizer_config_content, out_f, indent=2, ensure_ascii=False)\n",
    "        print(f\"파일 쓰기 완료: {output_filename}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"오류: '{tokenizer_json_path}' 파일의 JSON 내용을 디코딩할 수 없습니다. 파일이 손상되었거나 JSON 형식이 아닐 수 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 읽기 또는 출력 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(f\"오류: '{tokenizer_json_path}' 파일을 찾을 수 없습니다. 토크나이저가 올바르게 저장되었는지 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b80b8-4558-4a20-96e7-3eab5f0d2882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
