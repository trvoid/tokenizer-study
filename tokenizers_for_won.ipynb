{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 원불교 교전 데이터 대상 토크나이저 훈련\n",
        "\n",
        "## 1. 개요\n",
        "\n",
        "원불교 교전 데이터를 대상으로 토크나이저를 훈련합니다.\n",
        "\n",
        "* 데이터셋:\n",
        "  * 원불교 교전\n",
        "* 토크나이저:\n",
        "  * 허깅 페이스 [Tokenizers](https://huggingface.co/docs/tokenizers/index) 라이브러리의 [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer) 클래스"
      ],
      "metadata": {
        "id": "EaXjlgLSWSAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 말뭉치 준비"
      ],
      "metadata": {
        "id": "KCMFbIIYYehU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_file = \"./won/data/won04-gyojeon.txt\""
      ],
      "metadata": {
        "id": "JjHXMR1CYzE1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "말뭉치 파일에 대하여 아래 정보를 출력합니다.\n",
        "\n",
        "* 총 줄 수\n",
        "* 총 단어 수(단어를 공백으로 구분할 때)\n",
        "* 가장 많은 단어를 포함하고 있는 줄의 단어 수\n",
        "* 가장 많은 단어를 포함하고 있는 줄의 텍스트"
      ],
      "metadata": {
        "id": "ruXinS3Ok-nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_lines_and_words(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as file:\n",
        "            lines = file.readlines()\n",
        "            line_count = len(lines)\n",
        "            word_count = 0\n",
        "            max_line_length = 0\n",
        "            max_line = \"\"\n",
        "            for line in lines:\n",
        "                words = line.split() # 공백으로 단어 분리\n",
        "                line_length = len(words)\n",
        "                if line_length > max_line_length:\n",
        "                    max_line_length = line_length\n",
        "                    max_line = line\n",
        "                word_count += line_length\n",
        "            return line_count, word_count, max_line_length, max_line\n",
        "    except FileNotFoundError:\n",
        "        print(f\"오류: 파일 '{filepath}'를 찾을 수 없습니다.\")\n",
        "        return None, None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"파일을 읽는 중 오류가 발생했습니다: {e}\")\n",
        "        return None, None, None, None\n",
        "\n",
        "# 함수 호출 및 결과 출력\n",
        "lines, words, max_line_length, max_line = count_lines_and_words(corpus_file)\n",
        "\n",
        "if lines is not None and words is not None:\n",
        "    print(f\"파일 분석 결과:\")\n",
        "    print(f\"  * 총 줄 수  : {lines:8,}\")\n",
        "    print(f\"  * 총 단어 수: {words:8,}\")\n",
        "    print(\"  * 가장 많은 단어를 포함하고 있는 줄:\")\n",
        "    print(f\"    - 단어 수: {max_line_length:8,}\")\n",
        "    print(f\"    - 텍스트 : {max_line}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMZJP36cjePD",
        "outputId": "690624ce-37f8-4d95-a90a-083b9826e1de"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파일 분석 결과:\n",
            "  * 총 줄 수  :    1,082\n",
            "  * 총 단어 수:   59,541\n",
            "  * 가장 많은 단어를 포함하고 있는 줄:\n",
            "    - 단어 수:      362\n",
            "    - 텍스트 : 17.한 사람이 대종사께 뵈옵고 여러 가지로 담화하는 가운데 [전주․이리 사이의 경편철도(輕便鐵道)는 본래 전라도 각지의 부호들이 주식 출자로 경영하는 것이라, 그들은 언제나 그 경편차를 무료로 이용하고 다닌다.] 하면서 매우 부러워하는 태도를 보이거늘, 대종사 말씀하시기를 [그대는 참으로 가난하도다. 아직 그 차 하나를 그대의 소유로 삼지 못하였는가.] 그 사람이 놀라 여쭙기를 [경편차 하나를 소유하자면 상당한 돈이 있어야 할 것이온데 이 같은 무산자로서 어떻게 그것을 소유할 수 있사오리까.] 대종사 말씀하시기를 [그러므로, 그대를 가난한 사람이라 하였으며, 설사 그대가 경편차 하나를 소유하였다 할지라도 나는 그것으로 그대를 부유한 사람이라고는 아니할 것이니, 이제 나의 살림하는 이야기를 좀 들어보라. 나는 저 전주 경편차뿐 아니라 나라 안의 차와 세계의 모든 차까지도 다 내 것을 삼은지가 벌써 오래 되었노니, 그대는 이 소식을 아직도 모르는가.] 그 사람이 더욱 놀라 사뢰기를 [그 말씀은 실로 요량 밖의 교훈이시므로 어리석은 소견으로는 그 뜻을 살피지 못하겠나이다.] 대종사 말씀하시기를 [사람이 기차 하나를 자기의 소유로 하려면 거액(巨額)의 자금이 일시에 들어야 할 것이요, 운영하는 모든 책임을 직접 담당하여 많은 괴로움을 받아야 할 것이나, 나의 소유하는 법은 그와 달라서 단번에 거액을 들이지도 아니하며, 모든 운영의 책임을 직접 지지도 아니하고, 다만 어디를 가게 되면 그 때마다 얼마씩의 요금만 지불하고 나의 마음대로 이용하는 것이니, 주야로 쉬지 않고 우리 차를 운전하며, 우리 철도를 수선하며, 우리 사무를 관리하여 주는 모든 우리 일꾼들의 급료와 비용이 너무 싸지 아니한가. 또, 나는 저번에 서울에 가서 한양 공원에 올라가 산책하면서 맑은 공기를 한 없이 호흡도 하고 온 공원의 흥취를 다 같이 즐기기도 하였으되, 누가 우리를 가라는 법도 없고 다시 오지 말라는 말도 아니하였나니, 피서 지대에 정자 몇 간만 두어도 매년 적지 않은 수호비가 들 것인데, 우리는 그러지 아니하고도 그 좋은 공원을 충분히 내 것으로 이용하지 아니 하였는가. 대저, 세상 사람이 무엇이나 제 것을 삼으려는 본의는 다 자기의 편리를 취함이어늘 기차나 공원을 모두 다 이와 같이 이용할 대로 이용하였으니 어떻게 소유한들 이 위에 더 나은 방법이 있겠는가. 그러므로, 나는 이것을 모두 다 내 것이라고 하였으며, 그뿐 아니라 세상의 모든 것과 그 모든 것을 싣고 있는 대지 강산까지도 다 내 것을 삼아 두고, 경우에 따라 그것을 이용하되 경위에만 어긋나지 않게 하면 아무도 금하고 말리지 못하나니, 이 얼마나 너른 살림인가. 그러나, 속세 범상한 사람들은 기국(器局)이 좁아서 무엇이나 기어이 그것을 자기 앞에 갖다 놓기로만 위주하여 공연히 일 많고 걱정되고 책임 무거울 것을 취하기에 급급하나니, 이는 참으로 국한 없이 큰 본가 살림을 발견하지 못한 연고니라.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text_for_test = \"물질이 개벽되니 정신을 개벽하자\"\n",
        "\n",
        "sample_texts_for_test = [\n",
        "    \"물질이 개벽되니 정신을 개벽하자\",\n",
        "    \"응용(應用)하는 데 온전한 생각으로 취사하기를 주의할 것이요\",\n",
        "    \"19.대종사 말씀하시기를 [스승이 법을 새로 내는 일이나\"\n",
        "]"
      ],
      "metadata": {
        "id": "31f_qgfLY_C1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 토크나이저 훈련\n",
        "\n",
        "알고리즘과 단어 개수를 아래와 같이 변경해 가면서 훈련하고 각각의 조합으로 디렉토리 <*알고리즘*>_<*단어 개수*>를 만들어 훈련 결과를 저장합니다.\n",
        "\n",
        "* 알고리즘: BPE, Unigram, WordPiece\n",
        "* 단어 개수: 4000, 5000, 7000, 10000"
      ],
      "metadata": {
        "id": "Dp1RAyk7eoGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, Unigram, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "def create_tokenizer_and_trainer(model_name, vocab_size):\n",
        "    if model_name == \"bpe\":\n",
        "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "        trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                            vocab_size=vocab_size,\n",
        "                            limit_alphabet=limit_alphabet,\n",
        "                            min_frequency=min_frequency,\n",
        "                            show_progress=False)\n",
        "    elif model_name == \"unigram\":\n",
        "        tokenizer = Tokenizer(Unigram())\n",
        "        trainer = UnigramTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                            unk_token=\"[UNK]\",\n",
        "                            vocab_size=vocab_size,\n",
        "                            show_progress=False)\n",
        "    elif model_name == \"wordpiece\":\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "        trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
        "                            vocab_size=vocab_size,\n",
        "                            limit_alphabet=limit_alphabet,\n",
        "                            min_frequency=min_frequency,\n",
        "                            show_progress=False)\n",
        "\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    return tokenizer, trainer"
      ],
      "metadata": {
        "id": "rmOiOGkZeqWP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_tokenizer(\n",
        "    model_names,\n",
        "    vocab_sizes,\n",
        "    limit_alphabet,\n",
        "    min_frequency,\n",
        "    data_files,\n",
        "    output_dir\n",
        "):\n",
        "    for model_name in model_names:\n",
        "        for vocab_size in vocab_sizes:\n",
        "            # 토크나이저와 트레이너 생성\n",
        "            tokenizer, trainer = create_tokenizer_and_trainer(model_name, vocab_size)\n",
        "            # 훈련\n",
        "            tokenizer.train(data_files, trainer)\n",
        "            # 저장\n",
        "            tokenizer_dir = os.path.join(output_dir, f\"{model_name}_{vocab_size}\")\n",
        "            if not os.path.exists(tokenizer_dir):\n",
        "                os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "            tokenizer_file = os.path.join(tokenizer_dir, \"tokenizer.json\")\n",
        "            tokenizer.save(tokenizer_file)\n",
        "            print(f\"Tokenizer file: {tokenizer_file}\")"
      ],
      "metadata": {
        "id": "Ge5iixnyewlK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = [\n",
        "    corpus_file\n",
        "]\n",
        "\n",
        "output_dir = \"./won/tokenizers\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model_names = [\"bpe\", \"unigram\", \"wordpiece\"]\n",
        "vocab_sizes = [2000, 3000, 4000, 5000, 7000, 10000]\n",
        "limit_alphabet = 6000\n",
        "min_frequency = 5\n",
        "\n",
        "train_tokenizer(model_names, vocab_sizes, limit_alphabet, min_frequency, data_files, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wZDTUSp_Iq",
        "outputId": "1b1e6275-ea69-4fcb-d257-3b725e119718"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer file: ./won/tokenizers/bpe_2000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/bpe_3000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/bpe_4000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/bpe_5000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/bpe_7000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/bpe_10000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_2000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_3000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_4000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_5000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_7000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/unigram_10000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_2000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_3000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_4000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_5000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_7000/tokenizer.json\n",
            "Tokenizer file: ./won/tokenizers/wordpiece_10000/tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 토크나이저 사용"
      ],
      "metadata": {
        "id": "A5Pl9CGSfYLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, traceback\n",
        "from transformers import DistilBertTokenizerFast, PreTrainedTokenizerFast\n",
        "\n",
        "def tokenize_and_print(model_names, vocab_sizes, texts):\n",
        "    for text in texts:\n",
        "        print(\"#\" * 80)\n",
        "        print(\"TEXT: '\" + text + \"'\")\n",
        "        for model_name in model_names:\n",
        "            print(f\">>> {model_name}\")\n",
        "            for vocab_size in vocab_sizes:\n",
        "                try:\n",
        "                    tokenizer_file = os.path.join(output_dir, f\"{model_name}_{vocab_size}\", \"tokenizer.json\")\n",
        "                    tokenizer = Tokenizer.from_file(tokenizer_file)\n",
        "                    print(f\"{vocab_size:5}: {tokenizer.encode(text).tokens}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"{vocab_size:5}: FAIL - {e}\")\n",
        "                    traceback.print_exc(file=sys.stdout)"
      ],
      "metadata": {
        "id": "VlLdB4R6fRfB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_and_print(model_names, vocab_sizes, sample_texts_for_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6-7_VmIfdjI",
        "outputId": "de4ce178-c306-49d1-b2ca-eb41ee5092c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "################################################################################\n",
            "TEXT: '물질이 개벽되니 정신을 개벽하자'\n",
            ">>> bpe\n",
            " 2000: ['물', '질', '이', '개', '벽', '되', '니', '정신', '을', '개', '벽', '하', '자']\n",
            " 3000: ['물질', '이', '개', '벽', '되', '니', '정신을', '개', '벽', '하', '자']\n",
            " 4000: ['물질', '이', '개', '벽', '되니', '정신을', '개', '벽', '하', '자']\n",
            " 5000: ['물질', '이', '개벽', '되니', '정신을', '개벽', '하자']\n",
            " 7000: ['물질', '이', '개벽', '되니', '정신을', '개벽', '하자']\n",
            "10000: ['물질', '이', '개벽', '되니', '정신을', '개벽', '하자']\n",
            ">>> unigram\n",
            " 2000: ['물질', '이', '개', '벽', '되', '니', '정신', '을', '개', '벽', '하자']\n",
            " 3000: ['물질', '이', '개', '벽', '되', '니', '정신', '을', '개', '벽', '하자']\n",
            " 4000: ['물질', '이', '개벽', '되', '니', '정신', '을', '개벽', '하자']\n",
            " 5000: ['물질', '이', '개벽', '되니', '정신', '을', '개벽', '하자']\n",
            " 7000: ['물질이', '개벽', '되니', '정신', '을', '개벽', '하자']\n",
            "10000: ['물질이', '개벽', '되니', '정신', '을', '개벽', '하자']\n",
            ">>> wordpiece\n",
            " 2000: ['물', '##질', '##이', '개', '##벽', '##되', '##니', '정', '##신', '##을', '개', '##벽', '##하', '##자']\n",
            " 3000: ['물', '##질', '##이', '개', '##벽', '##되', '##니', '정', '##신', '##을', '개', '##벽', '##하', '##자']\n",
            " 4000: ['물질', '##이', '개', '##벽', '##되', '##니', '정신을', '개', '##벽', '##하', '##자']\n",
            " 5000: ['물질', '##이', '개', '##벽', '##되', '##니', '정신을', '개', '##벽', '##하', '##자']\n",
            " 7000: ['물질', '##이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자']\n",
            "10000: ['물질', '##이', '개벽', '##되', '##니', '정신을', '개벽', '##하', '##자']\n",
            "################################################################################\n",
            "TEXT: '응용(應用)하는 데 온전한 생각으로 취사하기를 주의할 것이요'\n",
            ">>> bpe\n",
            " 2000: ['응', '용', '(', '應', '用', ')', '하는', '데', '온', '전', '한', '생각', '으로', '취', '사', '하', '기를', '주의', '할', '것이요']\n",
            " 3000: ['응용', '(', '應', '用', ')', '하는', '데', '온전', '한', '생각', '으로', '취사', '하기를', '주의할', '것이요']\n",
            " 4000: ['응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '하기를', '주의할', '것이요']\n",
            " 5000: ['응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '하기를', '주의할', '것이요']\n",
            " 7000: ['응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '하기를', '주의할', '것이요']\n",
            "10000: ['응용', '(', '應', '用', ')', '하는', '데', '온전한', '생각으로', '취사', '하기를', '주의할', '것이요']\n",
            ">>> unigram\n",
            " 2000: ['응', '용', '(', '應', '用', ')', '하는', '데', '온', '전', '한', '생각', '으로', '취사', '하기', '를', '주', '의', '할', '것이', '요']\n",
            " 3000: ['응용', '(', '應', '用', ')', '하는', '데', '온전', '한', '생각', '으로', '취사', '하기', '를', '주의할', '것이', '요']\n",
            " 4000: ['응용', '(', '應用', ')', '하는', '데', '온전', '한', '생각으로', '취사', '하기', '를', '주의할', '것이', '요']\n",
            " 5000: ['응용', '(', '應用', ')', '하는', '데', '온전', '한', '생각으로', '취사하', '기', '를', '주의할', '것이', '요']\n",
            " 7000: ['응용', '(', '應用', ')', '하는', '데', '온전', '한', '생각으로', '취사하', '기', '를', '주의할', '것이', '요']\n",
            "10000: ['응용', '(', '應用', ')', '하는', '데', '온전', '한', '생각으로', '취사하', '기', '를', '주의할', '것이', '요']\n",
            ">>> wordpiece\n",
            " 2000: ['응', '##용', '(', '應', '##用', ')', '하', '##는', '데', '온', '##전', '##한', '생', '##각', '##으', '##로', '취', '##사', '##하', '##기', '##를', '주', '##의', '##할', '것', '##이', '##요']\n",
            " 3000: ['응', '##용', '(', '應', '##用', ')', '하', '##는', '데', '온', '##전', '##한', '생', '##각', '##으', '##로', '취', '##사', '##하', '##기', '##를', '주', '##의', '##할', '것', '##이', '##요']\n",
            " 4000: ['응용', '(', '應', '##用', ')', '하는', '데', '온', '##전', '##한', '생각', '##으로', '취사', '##하기를', '주의', '##할', '것이요']\n",
            " 5000: ['응용', '(', '應', '##用', ')', '하는', '데', '온전', '##한', '생각으로', '취사', '##하기를', '주의할', '것이요']\n",
            " 7000: ['응용', '(', '應', '##用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요']\n",
            "10000: ['응용', '(', '應', '##用', ')', '하는', '데', '온전한', '생각으로', '취사', '##하기를', '주의할', '것이요']\n",
            "################################################################################\n",
            "TEXT: '19.대종사 말씀하시기를 [스승이 법을 새로 내는 일이나'\n",
            ">>> bpe\n",
            " 2000: ['1', '9', '.', '대종사', '말씀하시기를', '[', '스', '승', '이', '법을', '새', '로', '내', '는', '일이', '나']\n",
            " 3000: ['19', '.', '대종사', '말씀하시기를', '[', '스승', '이', '법을', '새', '로', '내는', '일이', '나']\n",
            " 4000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            " 5000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            " 7000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            "10000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내는', '일이나']\n",
            ">>> unigram\n",
            " 2000: ['1', '9', '.', '대종사', '말씀하시', '기', '를', '[', '스승', '이', '법', '을', '새', '로', '내', '는', '일이', '나']\n",
            " 3000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승', '이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            " 4000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            " 5000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            " 7000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            "10000: ['19', '.', '대종사', '말씀하시', '기', '를', '[', '스승이', '법', '을', '새로', '내', '는', '일이', '나']\n",
            ">>> wordpiece\n",
            " 2000: ['1', '##9', '.', '대', '##종', '##사', '말', '##씀', '##하', '##시', '##기', '##를', '[', '스', '##승', '##이', '법', '##을', '새', '##로', '내', '##는', '일', '##이', '##나']\n",
            " 3000: ['1', '##9', '.', '대', '##종', '##사', '말', '##씀', '##하', '##시', '##기', '##를', '[', '스', '##승', '##이', '법', '##을', '새', '##로', '내', '##는', '일', '##이', '##나']\n",
            " 4000: ['1', '##9', '.', '대종사', '말씀하시기를', '[', '스승', '##이', '법을', '새', '##로', '내', '##는', '일이', '##나']\n",
            " 5000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내', '##는', '일이나']\n",
            " 7000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내', '##는', '일이나']\n",
            "10000: ['19', '.', '대종사', '말씀하시기를', '[', '스승이', '법을', '새로', '내', '##는', '일이나']\n"
          ]
        }
      ]
    }
  ]
}