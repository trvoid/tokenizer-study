{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d6b432",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/trvoid/tokenizer-study/blob/main/bert_word_piece_korquad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6337e7-a6de-42cd-810e-0e310d9b6285",
   "metadata": {
    "id": "5d6337e7-a6de-42cd-810e-0e310d9b6285"
   },
   "source": [
    "# BERT WordPiece 토크나이저 + KorQuAD 데이터셋 (한국어)\n",
    "\n",
    "## 1. 개요\n",
    "\n",
    "이 문서의 목적:\n",
    "* BERT WordPiece 토크나이저가 제공하는 여러 가지 옵션을 변경해 가면서 결과가 어떻게 달라지는지 파악합니다.\n",
    "\n",
    "모델:\n",
    "* [BertWordPieceTokenizer](https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/bert_wordpiece.py): BERT WordPiece Tokenizer Implementation. By Hugging Face.\n",
    "\n",
    "데이터셋:\n",
    "* [KorQuAD v1.0](https://korquad.github.io/category/1.0_KOR.html): The Korean Question Answering Dataset. By LG CNS AI Research Center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4VhV7irMBT6K",
   "metadata": {
    "id": "4VhV7irMBT6K"
   },
   "source": [
    "## 2. 토크나이저 훈련용 데이터 파일 만들기\n",
    "\n",
    "데이터셋을 다운로드하여 메모리에 적재하고 이로부터 토크나이저 훈련용 데이터 파일을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7MMPOn0BBXdD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7MMPOn0BBXdD",
    "outputId": "3298493f-5785-44b5-dd87-a36ae8c12f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5b2d78-ecae-4d1a-9b55-980b0063ef81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea5b2d78-ecae-4d1a-9b55-980b0063ef81",
    "outputId": "a54a8bbd-1e38-499c-dea7-cdf8644f2366"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since KorQuAD/squad_kor_v1 couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since KorQuAD/squad_kor_v1 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'squad_kor_v1' at /root/.cache/huggingface/datasets/KorQuAD___squad_kor_v1/squad_kor_v1/0.0.0/01aad23853355e5f4f6317eeaaa8186811424834 (last modified on Sat Apr 12 03:26:45 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'squad_kor_v1' at /root/.cache/huggingface/datasets/KorQuAD___squad_kor_v1/squad_kor_v1/0.0.0/01aad23853355e5f4f6317eeaaa8186811424834 (last modified on Sat Apr 12 03:26:45 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"KorQuAD/squad_kor_v1\"\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743ca671-2bdb-4f9c-94fa-8a5f7cf610e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "743ca671-2bdb-4f9c-94fa-8a5f7cf610e7",
    "outputId": "f2daa10c-7a33-4611-eb20-fbec1376c6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> context: 9월 26일 환경부를 비롯한 12개 관계부처가 합동으로 '미세먼지 관리 종합대책'을 확정하고 발전·산업·수송·생활 등 4개 부분에서 저감 대책을 실시하는 관련 로드맵을 발표했다. 7조 2000억 원의 예산을 투입해 미세먼지 국내 배출량을 30% 감축하고 미세먼지 '나쁨' 일수를 70%까지 줄이기로 한 것인데 이를 위해 공정률 10% 미만인 석탄발전소 9기 중 4기를 LNG 등 친환경 연료로 전환하고 남은 5기도 최고 수준의 배출 기준을 적용하며 30년이 넘은 노후 석탄발전소 7기는 임기 내 폐쇄하기로 했다. 또한 대기배출총량제를 전국으로 확대·강화하고 먼지총량제를 새로 도입하며, 노후 경유차 221만 대를 임기 내 77% 조기 폐차하고 친환경 차를 2022년까지 200만 대 보급하며 미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행하기로 했다. 국제적으로는 미세먼지를 한중 양국의 정상의제로 격상하고 동북아 지역에서 협약체결을 추진하면서 미세먼지 환경기준도 선진국 수준으로 강화할 것도 포함했다.'\n",
      "'>>> question: 미세먼지 해결을 위해 전국으로 확대 강화된 기존의 제도는?'\n",
      "'>>> answers: {'text': ['대기배출총량제'], 'answer_start': [290]}'\n",
      "\n",
      "'>>> context: 프리스틀리는 워링턴 거주 시절에는 다른 일 때문에 신학 연구에 몰두하지 못하였으나, 리즈에 오면서 그는 신학 연구에 많은 시간을 투자하였고, 결과적으로 그의 신앙은 아리우스주의에서 유니테리언으로 정립되었다. 리즈에서 프리스틀리는 삼위일체와 예수의 선재성(先在性, 성자인 예수는 천지창조전부터 성부와 같이 존재했다는 교리)등을 완전히 부정하였고, 기독교 교리와 성서를 새롭게 해석하기 시작했다. 그는 오래전부터 써오던 신학 교육에 대한 책인 《자연과 계시 종교의 원리》(Institutes of Natural and Revealed Religion)를 출판하기 시작하였는데, 1772년에 1권이 출판되었고 마지막 3권은 1774년에 출판되었다. 그는 책에서 자연 종교, 계시의 진실성을 뒷받침하는 논거, 계시로부터 얻을 수 있는 진실 등을 논했다. 이 책은 약 반세기간의 자유주의 신학자들의 주장을 요약하였고 오랜 기간에 걸쳐 유니테리언의 대표적인 해설서가 되었다.'\n",
      "'>>> question: 오랜 신학 연구 끝에 프리스틀리의 신앙은 아리우스주의를 거쳐 무엇으로 정립되었는가?'\n",
      "'>>> answers: {'text': ['유니테리언'], 'answer_start': [101]}'\n",
      "\n",
      "'>>> context: 그는 밤이 새도록 근로기준법 조문을 찾아 암기하며 현장에서 발생하는 노동자 불이익에 저항하였다. 그는 동대문구청을 찾아가 열악한 환경에 대해 호소했지만 받아들여지지 않았다. 그는 근로기준법상의 감독권 행사를 요청하기 위해 시청 근로감독관실을 찾아갔지만, 근로감독관은 평화 시장의 참혹한 얘기에 관심 조차 보이지 않았다. 그는 다시 노동청의 문을 두드렸지만 결과는 마찬가지였다. 가뜩이나 어려운 조건 속에서 발버둥치고 있던 전태일에게 그것은 너무나 큰 충격이었다. 노동이나 근로 기준법조차 사업주들의 편이라는 현실은 그를 한동안 허탈 상태로 몰아 넣었다. 그는 청계천 일대의 노동 실태를 직접 조사, 설문하여 이를 토대로 근로기준법 준수를 요구하는 청원서를 노동청에 냈으나 돌아온 답변은 경멸과 비웃음 뿐이었다. 처음에 약간 말투가 어눌했던 그는 부랑자로 몰리거나, 노동청 공무원들에게 조롱의 대상이 되기도 했다. 그의 노동자 권리 청원은 언론들도 외면했고, 경향신문 등에만 간략하게 보도되었다.'\n",
      "'>>> question: 그는 근로기준법상의 감독권 행사를 요청하려고 어느 곳을 찾아갔는가?'\n",
      "'>>> answers: {'text': ['시청 근로감독관실'], 'answer_start': [124]}'\n"
     ]
    }
   ],
   "source": [
    "samples = dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in samples:\n",
    "    print(f\"\\n'>>> context: {row['context']}'\")\n",
    "    print(f\"'>>> question: {row['question']}'\")\n",
    "    print(f\"'>>> answers: {row['answers']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eB0Pdg8CY4K",
   "metadata": {
    "id": "4eB0Pdg8CY4K"
   },
   "source": [
    "훈련용 데이터 파일을 네 개의 파일로 분리해서 만듭니다. 물론 하나의 파일로 만들어서 사용해도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eWIXO1yCZXE",
   "metadata": {
    "id": "7eWIXO1yCZXE"
   },
   "outputs": [],
   "source": [
    "with open('korquad_train_context.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(dataset[\"train\"][\"context\"]))\n",
    "with open('korquad_train_question.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(dataset[\"train\"][\"question\"]))\n",
    "\n",
    "with open('korquad_validation_context.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(dataset[\"validation\"][\"context\"]))\n",
    "with open('korquad_validation_question.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(\"\\n\".join(dataset[\"validation\"][\"question\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66823243-9d3a-4623-9cff-2176bc1dc78f",
   "metadata": {
    "id": "66823243-9d3a-4623-9cff-2176bc1dc78f"
   },
   "source": [
    "## 3. BertWordPieceTokenizer 훈련\n",
    "\n",
    "단어 사전 크기를 변경해 가면서 토크나이저 훈련을 수행하고 결과를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad71b2a1-e75f-4286-ac7d-f86f87b0f89d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad71b2a1-e75f-4286-ac7d-f86f87b0f89d",
    "outputId": "1b72f2da-dccd-4dc2-9b6e-89fbef026489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_5000.json\n",
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_10000.json\n",
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_20000.json\n",
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_30000.json\n",
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_40000.json\n",
      "Saved tokenizer: bert_word_piece_korquad/tokenizer_50000.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "data_files = [\n",
    "    \"korquad_train_context.txt\",\n",
    "    \"korquad_train_question.txt\",\n",
    "    \"korquad_validation_context.txt\",\n",
    "    \"korquad_validation_question.txt\"\n",
    "]\n",
    "\n",
    "output_dir = \"bert_word_piece_korquad\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "vocab_sizes = [5000, 10000, 20000, 30000, 40000, 50000]\n",
    "limit_alphabet = 6000\n",
    "min_frequency = 5\n",
    "\n",
    "for vocab_size in vocab_sizes:\n",
    "    tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "    tokenizer.train(files=data_files,\n",
    "                    vocab_size=vocab_size,\n",
    "                    limit_alphabet=limit_alphabet,\n",
    "                    min_frequency=min_frequency)\n",
    "\n",
    "    tokenizer_file = os.path.join(output_dir, f\"tokenizer_{vocab_size}.json\")\n",
    "    tokenizer.save(tokenizer_file)\n",
    "    print(f\"Saved tokenizer: {tokenizer_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcadea-620d-4db5-8d4c-740840f32676",
   "metadata": {
    "id": "3bfcadea-620d-4db5-8d4c-740840f32676"
   },
   "source": [
    "## 4. 훈련 결과 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce70d09-2b78-4635-8fd3-8c304f1dc405",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ce70d09-2b78-4635-8fd3-8c304f1dc405",
    "outputId": "03313ac5-58a5-49d2-969a-d9de5f95cbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "TEXT: 미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행\n",
      " 5000: ['미', '##세', '##먼', '##지', '##가', '심', '##하', '##면', '차', '##량', '2', '##부', '##제', '##와', '같', '##은', '비', '##상', '##저', '##감', '##조', '##치', '##를', '시', '##행']\n",
      "10000: ['미', '##세', '##먼', '##지가', '심', '##하면', '차량', '2', '##부', '##제와', '같은', '비', '##상', '##저', '##감', '##조', '##치를', '시행']\n",
      "20000: ['미세', '##먼', '##지가', '심', '##하면', '차량', '2부', '##제와', '같은', '비상', '##저', '##감', '##조', '##치를', '시행']\n",
      "30000: ['미세먼지', '##가', '심', '##하면', '차량', '2부', '##제와', '같은', '비상', '##저', '##감', '##조치를', '시행']\n",
      "40000: ['미세먼지', '##가', '심', '##하면', '차량', '2부', '##제와', '같은', '비상', '##저', '##감', '##조치를', '시행']\n",
      "50000: ['미세먼지', '##가', '심', '##하면', '차량', '2부', '##제와', '같은', '비상', '##저', '##감', '##조치를', '시행']\n",
      "################################################################################\n",
      "TEXT: 가뜩이나 어려운 조건 속에서\n",
      " 5000: ['가', '##뜩', '##이', '##나', '어', '##려', '##운', '조', '##건', '속', '##에', '##서']\n",
      "10000: ['가', '##뜩', '##이나', '어려', '##운', '조건', '속에서']\n",
      "20000: ['가', '##뜩', '##이나', '어려운', '조건', '속에서']\n",
      "30000: ['가', '##뜩', '##이나', '어려운', '조건', '속에서']\n",
      "40000: ['가', '##뜩', '##이나', '어려운', '조건', '속에서']\n",
      "50000: ['가', '##뜩', '##이나', '어려운', '조건', '속에서']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "texts = [\n",
    "    \"미세먼지가 심하면 차량 2부제와 같은 비상저감조치를 시행\",\n",
    "    \"가뜩이나 어려운 조건 속에서\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(\"#\" * 80)\n",
    "    print(\"TEXT: \" + text)\n",
    "    for vocab_size in vocab_sizes:\n",
    "        tokenizer_file = os.path.join(output_dir, f\"tokenizer_{vocab_size}.json\")\n",
    "        tokenizer = Tokenizer.from_file(tokenizer_file)\n",
    "        print(f\"{vocab_size:5}: {tokenizer.encode(text).tokens}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
